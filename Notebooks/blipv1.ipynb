{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:34:20.899816Z",
     "iopub.status.busy": "2025-04-07T18:34:20.899443Z",
     "iopub.status.idle": "2025-04-07T18:34:21.210228Z",
     "shell.execute_reply": "2025-04-07T18:34:21.209344Z",
     "shell.execute_reply.started": "2025-04-07T18:34:20.899791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Cluster similar classes into common one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:34:21.211738Z",
     "iopub.status.busy": "2025-04-07T18:34:21.211304Z",
     "iopub.status.idle": "2025-04-07T18:34:21.216330Z",
     "shell.execute_reply": "2025-04-07T18:34:21.215564Z",
     "shell.execute_reply.started": "2025-04-07T18:34:21.211683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "diagnosis_map = {\n",
    "    # Benign (0)\n",
    "    \"nevus\": 0, \"solar lentigo\": 0, \"dermatofibroma\": 0, \"vascular lesion\": 0, \"nev\": 0, \"sek\": 0,\n",
    "    \"seborrheic keratosis\": 0,\n",
    "    \"blue nevus\": 0, \"congenital nevus\": 0, \"dermal nevus\": 0, \"seborrheic keratosis\": 0,\n",
    "    \"nv\": 0, \"nevus\": 0, \"bkl\": 0, \"benign keratosis\": 0, \"df\": 0, \"dermatofibroma\": 0,\n",
    "    \"vasc\": 0, \"vascular\": 0,\n",
    "    \"Intradermal Nevus\": 0, \"common nevus\":0,\n",
    "    # Intermediate Benign (1)\n",
    "    \"atypical melanocytic proliferation\": 1, \"actinic keratosis\": 1, \"lichenoid keratosis\": 1,\n",
    "    \"ack\": 1, \"akiec\": 1, \"atypical nevus\":1,\n",
    "    # Intermediate Melanoma (2)\n",
    "    \"melanoma (in situ)\": 2, \"melanoma (<0.76 mm)\": 2, \"lentigo maligna\": 2,\n",
    "    \"atypical spitz tumor\": 2,\n",
    "    # Melanoma (3)\n",
    "    \"melanoma\": 3, \"melanoma metastasis\": 3, \"melanoma (>0.76 mm)\": 3,\n",
    "    \"mel\": 3, \"Nodular Melanoma\": 3, \"melanoma (0.76 to 1.5 mm)\": 3,\n",
    "    \"melanoma (more than 1.5 mm)\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 From .TXT to .CSV (For PH2 Dataset)\n",
    "This cell does the necessary string manupulation to adapt the .CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:34:21.217808Z",
     "iopub.status.busy": "2025-04-07T18:34:21.217522Z",
     "iopub.status.idle": "2025-04-07T18:34:21.293422Z",
     "shell.execute_reply": "2025-04-07T18:34:21.292707Z",
     "shell.execute_reply.started": "2025-04-07T18:34:21.217781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. CSV saved as PH2_dataset.csv with 220 rows\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "# Legend mappings remain the same\n",
    "clinical_diag_map = {\n",
    "    \"0\": \"Common Nevus\",\n",
    "    \"1\": \"Atypical Nevus\",\n",
    "    \"2\": \"Melanoma\"\n",
    "}\n",
    "\n",
    "asymmetry_map = {\n",
    "    \"0\": \"Fully Symmetric\",\n",
    "    \"1\": \"Symetric in 1 axe\",\n",
    "    \"2\": \"Fully Asymmetric\"\n",
    "}\n",
    "\n",
    "feature_map = {\n",
    "    \"A\": \"Absent\",\n",
    "    \"AT\": \"Atypical\",\n",
    "    \"P\": \"Present\",\n",
    "    \"T\": \"Typical\"\n",
    "}\n",
    "\n",
    "colors_map = {\n",
    "    \"1\": \"White\",\n",
    "    \"2\": \"Red\",\n",
    "    \"3\": \"Light-Brown\",\n",
    "    \"4\": \"Dark-Brown\",\n",
    "    \"5\": \"Blue-Gray\",\n",
    "    \"6\": \"Black\"\n",
    "}\n",
    "\n",
    "# Read the file\n",
    "with open(\"/kaggle/input/ph2dataset/PH2Dataset/PH2_dataset.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Get the header line\n",
    "header_line = lines[0].strip()\n",
    "header_line = re.sub(r'^\\|\\||\\|\\|$', '', header_line)  # Remove leading/trailing ||\n",
    "\n",
    "# Split by both single and double pipes\n",
    "header_parts = re.split(r'\\|\\||\\|', header_line)\n",
    "header = [part.strip() for part in header_parts if part.strip()]\n",
    "\n",
    "# Process data rows\n",
    "processed_rows = []\n",
    "for line in lines[1:]:\n",
    "    if not line.strip() or line.startswith(\"||---\"):\n",
    "        continue\n",
    "\n",
    "    # Clean the line\n",
    "    clean_line = line.strip()\n",
    "    clean_line = re.sub(r'^\\|\\||\\|\\|$', '', clean_line)  # Remove leading/trailing ||\n",
    "\n",
    "    # Split by both single and double pipes\n",
    "    parts = re.split(r'\\|\\||\\|', clean_line)\n",
    "    row_data = [part.strip() for part in parts]\n",
    "\n",
    "    # Create a dictionary for this row with all columns\n",
    "    row_dict = {}\n",
    "\n",
    "    # Add data for each column, using empty string for missing values\n",
    "    for i, field_name in enumerate(header):\n",
    "        if i >= len(row_data):\n",
    "            value = \"\"\n",
    "        elif i == 2 and row_data[i]:  # Clinical Diagnosis\n",
    "            value = clinical_diag_map.get(row_data[i], row_data[i])\n",
    "        elif i == 3 and row_data[i]:  # Asymmetry\n",
    "            value = asymmetry_map.get(row_data[i], row_data[i])\n",
    "        elif i >= 4 and i <= 8 and row_data[i]:  # Features\n",
    "            value = feature_map.get(row_data[i], row_data[i])\n",
    "        elif i == 9 and row_data[i]:  # Colors\n",
    "            value = \" \".join(colors_map.get(v, v) for v in row_data[i].split())\n",
    "        else:\n",
    "            value = row_data[i]\n",
    "\n",
    "        row_dict[field_name] = value\n",
    "\n",
    "    processed_rows.append(row_dict)\n",
    "\n",
    "# In convert-to-csv.py, modify the final section:\n",
    "# Write to CSV with only first 200 rows\n",
    "with open(\"PH2_dataset.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    for row in processed_rows[:200]:  # Only write first 200 rows\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Conversion complete. CSV saved as PH2_dataset.csv with {len(processed_rows)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🛠️ Define Paths & load metadata\n",
    "Sets file paths and load metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:34:21.294750Z",
     "iopub.status.busy": "2025-04-07T18:34:21.294494Z",
     "iopub.status.idle": "2025-04-07T18:34:21.413754Z",
     "shell.execute_reply": "2025-04-07T18:34:21.413066Z",
     "shell.execute_reply.started": "2025-04-07T18:34:21.294709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define dataset paths & load metadata\n",
    "base_dir = '/kaggle/input'\n",
    "FINAL_METADATA_PATH = \"/kaggle/working/unified_metadata.csv\"\n",
    "AUGMENTED_DATA_PATH = \"/kaggle/working/augmented_data/\"\n",
    "AUGMENTED_METADATA_PATH = \"/kaggle/working/augmented_metadata.csv\"\n",
    "TRAIN_METADATA_PATH = \"/kaggle/working/train_df.csv\"\n",
    "VAL_METADATA_PATH = \"/kaggle/working/validation_df.csv\"\n",
    "TEST_METADATA_PATH = \"/kaggle/working/test_df.csv\"\n",
    "\n",
    "pad_df = pd.read_csv('/kaggle/input/skin-cancer/metadata.csv')\n",
    "darm_df = pd.read_csv('/kaggle/input/derm7pt/release_v0/meta/meta.csv')\n",
    "ham_df = pd.read_csv('/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\n",
    "ph2_df = pd.read_csv('/kaggle/working/PH2_dataset.csv')\n",
    "\n",
    "ham_df.rename(columns={'image_id':'image_id', 'dx':'diagnosis'}, inplace=True)\n",
    "pad_df.rename(columns={'img_id':'image_id','diagnostic':'diagnosis'}, inplace=True)\n",
    "darm_df.rename(columns={'case_id':'image_id'}, inplace=True)\n",
    "ph2_df.rename(columns={'Name':'image_id', 'Clinical Diagnosis': 'diagnosis'}, inplace=True)\n",
    "\n",
    "ham_df['diagnosis_numeric'] = ham_df['diagnosis'].str.lower().map(diagnosis_map)\n",
    "pad_df['diagnosis_numeric'] = pad_df['diagnosis'].str.lower().map(diagnosis_map)\n",
    "darm_df['diagnosis_numeric'] = darm_df['diagnosis'].str.lower().map(diagnosis_map)\n",
    "ph2_df['diagnosis_numeric'] = ph2_df['diagnosis'].str.lower().map(diagnosis_map)\n",
    "\n",
    "ham_df['dataset_source'] = 'HAM10000'\n",
    "pad_df['dataset_source'] = 'PAD-UFES-20'\n",
    "darm_df['dataset_source'] = 'DERM7PT'\n",
    "ph2_df['dataset_source'] = 'PH2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Metadata modification & merging\n",
    "This cell filters out targeted columns & rows from all datasets and merging the all metadata files after preparing image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:34:21.414761Z",
     "iopub.status.busy": "2025-04-07T18:34:21.414502Z",
     "iopub.status.idle": "2025-04-07T18:34:54.671747Z",
     "shell.execute_reply": "2025-04-07T18:34:54.670909Z",
     "shell.execute_reply.started": "2025-04-07T18:34:21.414714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HAM10000' 'PAD-UFES-20' 'DERM7PT' 'PH2']\n",
      "['HAM10000' 'PAD-UFES-20' 'DERM7PT' 'PH2']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Merge explicitly\n",
    "unified_df = pd.concat([ham_df, pad_df, darm_df, ph2_df], ignore_index=True)\n",
    "unified_df.dropna(subset=['diagnosis_numeric'], inplace=True)\n",
    "print(unified_df[\"dataset_source\"].unique())\n",
    "\n",
    "# Explicit image path generation\n",
    "def generate_image_path(row):\n",
    "    source, image_id = row['dataset_source'], row['image_id']\n",
    "\n",
    "    if source == 'HAM10000':\n",
    "        for part in ['HAM10000_images_part_1', 'HAM10000_images_part_2']:\n",
    "            path = f\"{base_dir}/skin-cancer-mnist-ham10000/{part}/{image_id}.jpg\"\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "\n",
    "    elif source == 'PAD-UFES-20':\n",
    "        for part in [1,2,3]:\n",
    "            path = f\"{base_dir}/skin-cancer/imgs_part_{part}/imgs_part_{part}/{image_id}\"\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "\n",
    "    elif source == 'DERM7PT':\n",
    "        if pd.notnull(row['derm']):\n",
    "            return f\"{base_dir}/derm7pt/release_v0/images/{row['derm']}\"\n",
    "        elif pd.notnull(row['clinic']):\n",
    "            return f\"{base_dir}/derm7pt/release_v0/images/{row['clinic']}\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    elif source == 'PH2':\n",
    "        return f\"{base_dir}/ph2dataset/PH2Dataset/PH2_Dataset_images/{image_id}/{image_id}_Dermoscopic_Image/{image_id}.bmp\"\n",
    "\n",
    "unified_df['image_path'] = unified_df.apply(generate_image_path, axis=1)\n",
    "unified_df.dropna(subset=['image_path', 'diagnosis_numeric'], inplace=True)\n",
    "print(unified_df[\"dataset_source\"].unique())\n",
    "columns_to_keep = ['image_id', 'diagnosis', 'diagnosis_numeric', 'dataset_source', 'image_path']\n",
    "unified_df = unified_df[columns_to_keep]\n",
    "unified_df.to_csv(FINAL_METADATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Augmentation Definition\n",
    "This cell does the augmentation & saves the augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:34:54.672909Z",
     "iopub.status.busy": "2025-04-07T18:34:54.672577Z",
     "iopub.status.idle": "2025-04-07T18:34:58.880768Z",
     "shell.execute_reply": "2025-04-07T18:34:58.880055Z",
     "shell.execute_reply.started": "2025-04-07T18:34:54.672874Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def generate_augmented_df(original_df, target_count, transform, save_dir):\n",
    "    \"\"\"\n",
    "    Generates augmented images and returns new DataFrame with paths & labels.\n",
    "    Saves images to disk in save_dir.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    augmented_records = []\n",
    "    existing_count = len(original_df)\n",
    "    needed = target_count - existing_count\n",
    "\n",
    "    print(f\"Original: {existing_count}, Target: {target_count}, Augmenting: {needed}\")\n",
    "\n",
    "    augment_idx = 0\n",
    "    while len(augmented_records) < needed:\n",
    "        for idx, row in original_df.iterrows():\n",
    "            if len(augmented_records) >= needed:\n",
    "                break\n",
    "\n",
    "            img_path = row['image_path']\n",
    "            label = row['diagnosis_numeric']\n",
    "\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            augmented = transform(image=image)['image']\n",
    "\n",
    "            new_filename = f\"aug_{label}_{augment_idx}.jpg\"\n",
    "            save_path = os.path.join(save_dir, new_filename)\n",
    "            aug_img_np = augmented.permute(1, 2, 0).cpu().numpy()\n",
    "            aug_img_np = np.clip(aug_img_np * 255.0, 0, 255).astype(np.uint8)\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(aug_img_np, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            augmented_records.append({'image_path': save_path, 'diagnosis_numeric': label, 'original_image_path': img_path})\n",
    "            augment_idx += 1\n",
    "\n",
    "    new_df = pd.concat([original_df, pd.DataFrame(augmented_records)], ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "def balance_custom_classes(df, transform, save_root):\n",
    "    \"\"\"\n",
    "    Custom-balanced class augmentation strategy:\n",
    "    Benign -> 7000 (downsample)\n",
    "    Melanoma -> 5000 (augment)\n",
    "    Intermediate Benign -> 3000 (augment)\n",
    "    Intermediate Melanoma -> 1000 (augment)\n",
    "    \"\"\"\n",
    "    class_targets = {\n",
    "        0: 7000,  # Benign\n",
    "        3: 5000,  # Melanoma\n",
    "        1: 3000,  # Intermediate Benign\n",
    "        2: 1000   # Intermediate Melanoma\n",
    "    }\n",
    "\n",
    "    final_df_list = []\n",
    "\n",
    "    for cls, target_count in class_targets.items():\n",
    "        class_df = df[df['diagnosis_numeric'] == cls]\n",
    "        existing_count = len(class_df)\n",
    "\n",
    "        print(f\"\\nClass {cls}: Existing samples = {existing_count}\")\n",
    "\n",
    "        if existing_count > target_count:\n",
    "            class_df = class_df.sample(target_count, random_state=42).reset_index(drop=True)\n",
    "            print(f\"Downsampled to {target_count}\")\n",
    "            final_df_list.append(class_df)\n",
    "\n",
    "        elif existing_count < target_count:\n",
    "            save_dir = os.path.join(save_root, f\"aug_class_{cls}\")\n",
    "            class_aug_df = generate_augmented_df(class_df, target_count, transform, save_dir)\n",
    "            final_df_list.append(class_aug_df)\n",
    "\n",
    "        else:\n",
    "            final_df_list.append(class_df)\n",
    "\n",
    "    final_balanced_df = pd.concat(final_df_list, ignore_index=True)\n",
    "    return final_balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚙️ Augmentation Code Execution\n",
    "Executes a general part of the augmentation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:34:58.881845Z",
     "iopub.status.busy": "2025-04-07T18:34:58.881458Z",
     "iopub.status.idle": "2025-04-07T18:37:15.531669Z",
     "shell.execute_reply": "2025-04-07T18:37:15.531001Z",
     "shell.execute_reply.started": "2025-04-07T18:34:58.881821Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class 0: Existing samples = 8792\n",
      "Downsampled to 7000\n",
      "\n",
      "Class 3: Existing samples = 1291\n",
      "Original: 1291, Target: 5000, Augmenting: 3709\n",
      "\n",
      "Class 1: Existing samples = 1137\n",
      "Original: 1137, Target: 3000, Augmenting: 1863\n",
      "\n",
      "Class 2: Existing samples = 64\n",
      "Original: 64, Target: 1000, Augmenting: 936\n"
     ]
    }
   ],
   "source": [
    "# Augmentation to apply to the intermediate classes\n",
    "augment_pipeline = A.Compose([\n",
    "    A.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=30),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ColorJitter(p=0.3),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "df = pd.read_csv(FINAL_METADATA_PATH)\n",
    "\n",
    "augmented_metadata_df = balance_custom_classes(df, transform=augment_pipeline, save_root=AUGMENTED_DATA_PATH)\n",
    "augmented_metadata_df.to_csv(AUGMENTED_METADATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Prompt creation function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-07T18:37:15.534425Z",
     "iopub.status.busy": "2025-04-07T18:37:15.534202Z",
     "iopub.status.idle": "2025-04-07T18:37:15.546560Z",
     "shell.execute_reply": "2025-04-07T18:37:15.545820Z",
     "shell.execute_reply.started": "2025-04-07T18:37:15.534405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_descriptive_prompt(row):\n",
    "    parts = []\n",
    "\n",
    "    # Diagnosis label mapping for readability\n",
    "    diagnosis_mapping = {\n",
    "        \"bkl\": \"benign keratosis-like lesion\",\n",
    "        \"nv\": \"melanocytic nevus\",\n",
    "        \"mel\": \"melanoma\",\n",
    "        \"bcc\": \"basal cell carcinoma\",\n",
    "        \"akiec\": \"actinic keratosis\",\n",
    "        \"vasc\": \"vascular lesion\",\n",
    "        \"df\": \"dermatofibroma\"\n",
    "    }\n",
    "\n",
    "    # General patient info\n",
    "    sex = str(row.get(\"sex\") or row.get(\"gender\", \"\")).strip().lower()\n",
    "    age = row.get(\"age\") or row.get(\"Age\")\n",
    "    location = row.get(\"localization\") or row.get(\"location\")\n",
    "    raw_diagnosis = row.get(\"diagnosis\") or row.get(\"dx\") or row.get(\"Histological Diagnosis\") or row.get(\"Clinical Diagnosis\")\n",
    "    diagnosis = diagnosis_mapping.get(str(raw_diagnosis).lower(), str(raw_diagnosis).lower()) if pd.notna(raw_diagnosis) else None\n",
    "\n",
    "    intro = []\n",
    "    has_patient_info = False\n",
    "\n",
    "    if sex and sex != 'nan':\n",
    "        intro.append(f\"Patient is {sex}\")\n",
    "        has_patient_info = True\n",
    "    if pd.notna(age):\n",
    "        try:\n",
    "            age_val = int(float(age))\n",
    "            if sex and sex != 'nan':\n",
    "                intro.append(f\"aged {age_val}\")\n",
    "            else:\n",
    "                intro.append(f\"Patient age is {age_val}\")\n",
    "            has_patient_info = True\n",
    "        except:\n",
    "            pass\n",
    "    if pd.notna(location):\n",
    "        intro.append(f\"with a lesion on the {location.lower()}\")\n",
    "        has_patient_info = True\n",
    "\n",
    "    if intro:\n",
    "        if has_patient_info:\n",
    "            parts.append(\" \".join(intro) + \".\")\n",
    "        else:\n",
    "            diagnosis_phrase = [i for i in intro if \"diagnosed as\" in i]\n",
    "            if diagnosis_phrase:\n",
    "                parts.append(f\"This image shows a lesion {diagnosis_phrase[0]}.\")\n",
    "\n",
    "    # Symptoms (PAD)\n",
    "    for symptom in [\"itch\", \"hurt\", \"grew\", \"changed\", \"bleed\"]:\n",
    "        val = str(row.get(symptom, \"\")).lower()\n",
    "        if val in [\"1\", \"true\", \"yes\", \"t\", \"y\"]:\n",
    "            parts.append(f\"Patient reported that the lesion {symptom}s.\")\n",
    "\n",
    "    # PH2-specific structured features (grouped)\n",
    "    ph2_present = []\n",
    "    ph2_absent = []\n",
    "\n",
    "    for field in [\"Asymmetry\", \"Pigment Network\", \"Dots/Globules\", \"Streaks\",\n",
    "                  \"Regression Areas\", \"Blue-Whitish Veil\"]:\n",
    "        val = str(row.get(field, \"\")).strip()\n",
    "        if val:\n",
    "            if val.lower() == \"absent\":\n",
    "                ph2_absent.append(field.lower())\n",
    "            else:\n",
    "                ph2_present.append(f\"{field.lower()} is {val.lower()}\")\n",
    "\n",
    "    if pd.notna(row.get(\"Colors\")):\n",
    "        ph2_present.append(f\"colors observed include {row['Colors'].lower()}\")\n",
    "\n",
    "    if ph2_present:\n",
    "        parts.append(\"The lesion presents the following characteristics: \" + \", \".join(ph2_present) + \".\")\n",
    "    if ph2_absent:\n",
    "        parts.append(f\"Other features such as {', '.join(ph2_absent)} are absent.\")\n",
    "\n",
    "    # DARM features\n",
    "    darm_present = []\n",
    "    darm_absent = []\n",
    "\n",
    "    for field in [\"pigment_network\", \"streaks\", \"pigmentation\", \"regression_structures\",\n",
    "                  \"dots_and_globules\", \"blue_whitish_veil\", \"vascular_structures\"]:\n",
    "        if field in row and pd.notna(row[field]):\n",
    "            val = str(row[field]).strip().lower()\n",
    "            name = field.replace('_', ' ')\n",
    "            if val == \"absent\":\n",
    "                darm_absent.append(name)\n",
    "            else:\n",
    "                darm_present.append(f\"{val} {name}\")\n",
    "\n",
    "    if darm_present:\n",
    "        parts.append(f\"Dermoscopic features include {', '.join(darm_present)}.\")\n",
    "    if darm_absent:\n",
    "        parts.append(f\"Other features such as {', '.join(darm_absent)} are absent.\")\n",
    "\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def generate_text_prompts(df):\n",
    "    df = df.copy()\n",
    "    df['text_prompt'] = df.apply(create_descriptive_prompt, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Image path creation function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:37:15.547910Z",
     "iopub.status.busy": "2025-04-07T18:37:15.547647Z",
     "iopub.status.idle": "2025-04-07T18:37:15.575175Z",
     "shell.execute_reply": "2025-04-07T18:37:15.574427Z",
     "shell.execute_reply.started": "2025-04-07T18:37:15.547888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_image_path(row):\n",
    "    source, image_id = row['dataset_source'], row['image_id']\n",
    "\n",
    "    if source == 'HAM10000':\n",
    "        for part in ['HAM10000_images_part_1', 'HAM10000_images_part_2']:\n",
    "            path = f\"{base_dir}/skin-cancer-mnist-ham10000/{part}/{image_id}.jpg\"\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "\n",
    "    elif source == 'PAD-UFES-20':\n",
    "        for part in [1,2,3]:\n",
    "            path = f\"{base_dir}/skin-cancer/imgs_part_{part}/imgs_part_{part}/{image_id}\"\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "\n",
    "    elif source == 'DERM7PT':\n",
    "        if pd.notnull(row['derm']):\n",
    "            return f\"{base_dir}/derm7pt/release_v0/images/{row['derm']}\"\n",
    "        elif pd.notnull(row['clinic']):\n",
    "            return f\"{base_dir}/derm7pt/release_v0/images/{row['clinic']}\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    elif source == 'PH2':\n",
    "        return f\"{base_dir}/ph2dataset/PH2Dataset/PH2_Dataset_images/{image_id}/{image_id}_Dermoscopic_Image/{image_id}.bmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚙️ Augmentation, prompt creation & metadata finalization execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:37:15.576403Z",
     "iopub.status.busy": "2025-04-07T18:37:15.576107Z",
     "iopub.status.idle": "2025-04-07T18:37:30.548348Z",
     "shell.execute_reply": "2025-04-07T18:37:30.547682Z",
     "shell.execute_reply.started": "2025-04-07T18:37:15.576376Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified dataset preview:\n",
      "                image_id                                                                                                                                                                                     text_prompt                                                               image_path\n",
      "0  PAT_1516_1765_530.png                                                                                                                                                                               Patient age is 8.  /kaggle/input/skin-cancer/imgs_part_3/imgs_part_3/PAT_1516_1765_530.png\n",
      "1     PAT_46_881_939.png  Patient is female aged 55. Patient reported that the lesion itchs. Patient reported that the lesion grews. Patient reported that the lesion changeds. Patient reported that the lesion bleeds.     /kaggle/input/skin-cancer/imgs_part_1/imgs_part_1/PAT_46_881_939.png\n",
      "2  PAT_1545_1867_547.png                                                                                                                                      Patient age is 77. Patient reported that the lesion itchs.  /kaggle/input/skin-cancer/imgs_part_3/imgs_part_3/PAT_1545_1867_547.png\n",
      "3  PAT_1989_4061_934.png                                                                                                                                      Patient age is 75. Patient reported that the lesion itchs.  /kaggle/input/skin-cancer/imgs_part_3/imgs_part_3/PAT_1989_4061_934.png\n",
      "4   PAT_684_1302_588.png                                               Patient is male aged 79. Patient reported that the lesion itchs. Patient reported that the lesion grews. Patient reported that the lesion bleeds.   /kaggle/input/skin-cancer/imgs_part_2/imgs_part_2/PAT_684_1302_588.png\n"
     ]
    }
   ],
   "source": [
    "pad_df = generate_text_prompts(pad_df)\n",
    "darm_df = generate_text_prompts(darm_df)\n",
    "ham_df = generate_text_prompts(ham_df)\n",
    "ph2_df = generate_text_prompts(ph2_df)\n",
    "\n",
    "# Combine all into one final DataFrame\n",
    "original_text_prompt_df = pd.concat([pad_df, darm_df, ham_df, ph2_df], ignore_index=True)\n",
    "original_text_prompt_df['image_path'] = original_text_prompt_df.apply(generate_image_path, axis=1)\n",
    "print(\"Unified dataset preview:\")\n",
    "print(original_text_prompt_df[['image_id', 'text_prompt', 'image_path']].head().to_string())\n",
    "original_text_prompt_df = original_text_prompt_df.dropna(subset=['diagnosis_numeric'])\n",
    "\n",
    "augmented_metadata_df = pd.read_csv(AUGMENTED_METADATA_PATH)\n",
    "\n",
    "augmented_metadata_df.loc[augmented_metadata_df['original_image_path'].isna() |\n",
    "                         (augmented_metadata_df['original_image_path'] == ''),\n",
    "                         'original_image_path'] = augmented_metadata_df['image_path']\n",
    "\n",
    "merge_df = augmented_metadata_df.merge(\n",
    "    original_text_prompt_df[['image_path', 'text_prompt']],\n",
    "    left_on='original_image_path',\n",
    "    right_on='image_path',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "augmented_text_prompt_df = merge_df[['diagnosis', 'diagnosis_numeric', 'dataset_source', 'original_image_path', 'text_prompt']]\n",
    "augmented_text_prompt_df = augmented_text_prompt_df.rename(columns={\n",
    "    'original_image_path': 'image_path'\n",
    "})\n",
    "\n",
    "original_text_prompt_df.to_csv(\"original_vlm_with_text_prompt_image_path.csv\", index=False)\n",
    "augmented_text_prompt_df.to_csv(\"augmented_vlm_with_text_prompt_image_path.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚙️ Dataloader & Hyperparameters definition, dataset splitting, training & test part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:37:30.549544Z",
     "iopub.status.busy": "2025-04-07T18:37:30.549207Z",
     "iopub.status.idle": "2025-04-07T18:38:03.467702Z",
     "shell.execute_reply": "2025-04-07T18:38:03.465666Z",
     "shell.execute_reply.started": "2025-04-07T18:37:30.549513Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1656b08005dd4380975a348fdb03fe64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb955f2e6b34c0d9150db0ec588ef0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6129accc4e34677a39d65a4be37f77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b2cfffdadc4ce88f94cb3b4f7b612a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1420326dde4dc0986ec92353e82a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5aa100ac0c84215a90b0d3a865fd393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924b44f14c264564a927814787b8c3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/90 - Training:   0%|          | 1/700 [00:03<46:05,  3.96s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 178.12 MiB is free. Process 2773 has 14.56 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 184.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d2888c804696>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{EPOCHS} - Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         outputs = model(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         vision_outputs = self.vision_model(\n\u001b[0m\u001b[1;32m    985\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    727\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs_embeds, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    663\u001b[0m                 )\n\u001b[1;32m    664\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m                 layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m    666\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         hidden_states, attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 178.12 MiB is free. Process 2773 has 14.56 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 184.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# BLIP-style image-text classification for diagnosis labels\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BlipProcessor, BlipModel\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"/kaggle/working/augmented_vlm_with_text_prompt_image_path.csv\"\n",
    "MODEL_NAME = \"Salesforce/blip-image-captioning-base\"\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 30\n",
    "LR = 1e-4\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAVE_PATH = \"blip_classifier.pt\"\n",
    "\n",
    "# === LOAD CSV ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "labels = df['diagnosis_numeric'].values\n",
    "classes = np.unique(labels)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "# === DATASET ===\n",
    "class BlipDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row['image_path']).convert(\"RGB\")\n",
    "        prompt = str(row['text_prompt'])\n",
    "        label = int(row['diagnosis_numeric'])\n",
    "\n",
    "        inputs = self.processor(images=image, text=prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs['label'] = torch.tensor(label, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "# === INIT MODEL ===\n",
    "processor = BlipProcessor.from_pretrained(MODEL_NAME)\n",
    "model = BlipModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "classifier = torch.nn.Linear(model.config.text_config.hidden_size, len(classes)).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# === SPLIT DATA ===\n",
    "total_size = len(df)\n",
    "test_size = int(TEST_SPLIT * total_size)\n",
    "val_size = int(VAL_SPLIT * total_size)\n",
    "train_size = total_size - val_size - test_size\n",
    "train_df, val_df, test_df = random_split(df, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(BlipDataset(train_df.dataset.iloc[train_df.indices], processor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(BlipDataset(val_df.dataset.iloc[val_df.indices], processor), batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(BlipDataset(test_df.dataset.iloc[test_df.indices], processor), batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# === CHECKPOINT RESUME SETUP ===\n",
    "CHECKPOINT_PATH = \"checkpoint_epoch_38.pth\"\n",
    "start_epoch = 0\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    classifier.load_state_dict(checkpoint['classifier_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "# === TRAINING LOOP ===\n",
    "best_val_accuracy = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\"):\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'].to(DEVICE),\n",
    "            attention_mask=batch['attention_mask'].to(DEVICE),\n",
    "            pixel_values=batch['pixel_values'].to(DEVICE),\n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden = outputs.text_model_output.last_hidden_state[:, 0, :]\n",
    "        logits = classifier(hidden)\n",
    "        loss = criterion(logits, batch['label'].to(DEVICE))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # === VALIDATION ===\n",
    "    classifier.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'].to(DEVICE),\n",
    "                attention_mask=batch['attention_mask'].to(DEVICE),\n",
    "                pixel_values=batch['pixel_values'].to(DEVICE),\n",
    "                return_dict=True\n",
    "            )\n",
    "            hidden = outputs.text_model_output.last_hidden_state[:, 0, :]\n",
    "            logits = classifier(hidden)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "    acc = np.mean(np.array(val_preds) == np.array(val_labels))\n",
    "    precision = precision_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(val_labels, torch.nn.functional.one_hot(torch.tensor(val_preds), num_classes=len(classes)), average='weighted', multi_class='ovr')\n",
    "    except:\n",
    "        auc = 0.0\n",
    "\n",
    "    print(f\"Validation Accuracy: {acc:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
    "\n",
    "    if acc > best_val_accuracy:\n",
    "        best_val_accuracy = acc\n",
    "        torch.save(classifier.state_dict(), SAVE_PATH)\n",
    "        print(\"Saved best model.\")\n",
    "\n",
    "    # Save checkpoint every epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'classifier_state_dict': classifier.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()\n",
    "    }, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "\n",
    "    scheduler.step()\n",
    "    new_lr = scheduler.get_last_lr()\n",
    "    print(f\"Epoch {epoch+1}, Updated Learning Rate: {new_lr}\")\n",
    "\n",
    "# === TESTING ===\n",
    "classifier.load_state_dict(torch.load(SAVE_PATH))\n",
    "classifier.eval()\n",
    "test_preds, test_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'].to(DEVICE),\n",
    "            attention_mask=batch['attention_mask'].to(DEVICE),\n",
    "            pixel_values=batch['pixel_values'].to(DEVICE),\n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden = outputs.text_model_output.last_hidden_state[:, 0, :]\n",
    "        logits = classifier(hidden)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "print(\"\\n=== Test Set Performance ===\")\n",
    "print(classification_report(test_labels, test_preds, target_names=[str(c) for c in classes]))\n",
    "\n",
    "precision = precision_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
    "recall = recall_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
    "f1 = f1_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
    "try:\n",
    "    auc = roc_auc_score(test_labels, torch.nn.functional.one_hot(torch.tensor(test_preds), num_classes=len(classes)), average='weighted', multi_class='ovr')\n",
    "except:\n",
    "    auc = 0.0\n",
    "\n",
    "print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3904493,
     "sourceId": 6785866,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4259963,
     "sourceId": 7337662,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6892992,
     "sourceId": 11062562,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
