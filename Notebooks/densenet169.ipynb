{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🛠️ Define Paths and Hyperparameters\n",
    "Sets file paths and model training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.620Z"
    },
    "id": "Qam1sTsi-8oa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define dataset paths and training hyperparameters\n",
    "PH2_DATASET_TEXT_PATH = \"/kaggle/input/ph2dataset/PH2Dataset/PH2_dataset.txt\"\n",
    "PH2_DATASET_GENERATED_CSV_PATH = \"/kaggle/working/PH2_dataset.csv\"\n",
    "HAM_METADATA_PATH = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\n",
    "PAD_METADATA_PATH = \"/kaggle/input/skin-cancer/metadata.csv\"\n",
    "DARM_METADATA_PATH = \"/kaggle/input/derm7pt/release_v0/meta/meta.csv\"\n",
    "BASE_DIR = \"/kaggle/input\"\n",
    "FINAL_METADATA_PATH = \"/kaggle/working/unified_metadata.csv\"\n",
    "AUGMENTED_DATA_PATH = \"/kaggle/working/augmented_data/\"\n",
    "AUGMENTED_METADATA_PATH = \"/kaggle/working/augmented_metadata.csv\"\n",
    "TRAIN_METADATA_PATH = \"/kaggle/working/train_df.csv\"\n",
    "VAL_METADATA_PATH = \"/kaggle/working/validation_df.csv\"\n",
    "TEST_METADATA_PATH = \"/kaggle/working/test_df.csv\"\n",
    "\n",
    "BATCH = 32\n",
    "EPOCH = 160\n",
    "NUM_WORKER = 4\n",
    "LR = 3e-3\n",
    "ETA_MIN = 1e-5\n",
    "T_MAX = 120\n",
    "WEIGHT_DECAY = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📁 Setup Directory\n",
    "Cleans and prepares the directory for storing augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.622Z"
    },
    "id": "WBktZUkyd4eE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clean up and create the augmented data directory\n",
    "!rm -rf /kaggle/working/augmented_data\n",
    "!mkdir /kaggle/working/augmented_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 From .TXT to .CSV (For PH2 Dataset)\n",
    "This cell does the necessary string manupulation to adapt the .CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.622Z"
    },
    "id": "w7gjQzrRxWiF",
    "outputId": "d3b27c27-8994-47db-b0bb-1a17b4df6d0b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Legend mappings remain the same\n",
    "clinical_diag_map = {\n",
    "    \"0\": \"Common Nevus\",\n",
    "    \"1\": \"Atypical Nevus\",\n",
    "    \"2\": \"Melanoma\"\n",
    "}\n",
    "\n",
    "asymmetry_map = {\n",
    "    \"0\": \"Fully Symmetric\",\n",
    "    \"1\": \"Symetric in 1 axe\",\n",
    "    \"2\": \"Fully Asymmetric\"\n",
    "}\n",
    "\n",
    "feature_map = {\n",
    "    \"A\": \"Absent\",\n",
    "    \"AT\": \"Atypical\",\n",
    "    \"P\": \"Present\",\n",
    "    \"T\": \"Typical\"\n",
    "}\n",
    "\n",
    "colors_map = {\n",
    "    \"1\": \"White\",\n",
    "    \"2\": \"Red\",\n",
    "    \"3\": \"Light-Brown\",\n",
    "    \"4\": \"Dark-Brown\",\n",
    "    \"5\": \"Blue-Gray\",\n",
    "    \"6\": \"Black\"\n",
    "}\n",
    "\n",
    "# Read the file\n",
    "with open(PH2_DATASET_TEXT_PATH, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Get the header line\n",
    "header_line = lines[0].strip()\n",
    "header_line = re.sub(r'^\\|\\||\\|\\|$', '', header_line)  # Remove leading/trailing ||\n",
    "\n",
    "# Split by both single and double pipes\n",
    "header_parts = re.split(r'\\|\\||\\|', header_line)\n",
    "header = [part.strip() for part in header_parts if part.strip()]\n",
    "\n",
    "# Process data rows\n",
    "processed_rows = []\n",
    "for line in lines[1:]:\n",
    "    if not line.strip() or line.startswith(\"||---\"):\n",
    "        continue\n",
    "\n",
    "    # Clean the line\n",
    "    clean_line = line.strip()\n",
    "    clean_line = re.sub(r'^\\|\\||\\|\\|$', '', clean_line)  # Remove leading/trailing ||\n",
    "\n",
    "    # Split by both single and double pipes\n",
    "    parts = re.split(r'\\|\\||\\|', clean_line)\n",
    "    row_data = [part.strip() for part in parts]\n",
    "\n",
    "    # Create a dictionary for this row with all columns\n",
    "    row_dict = {}\n",
    "\n",
    "    # Add data for each column, using empty string for missing values\n",
    "    for i, field_name in enumerate(header):\n",
    "        if i >= len(row_data):\n",
    "            value = \"\"\n",
    "        elif i == 2 and row_data[i]:  # Clinical Diagnosis\n",
    "            value = clinical_diag_map.get(row_data[i], row_data[i])\n",
    "        elif i == 3 and row_data[i]:  # Asymmetry\n",
    "            value = asymmetry_map.get(row_data[i], row_data[i])\n",
    "        elif i >= 4 and i <= 8 and row_data[i]:  # Features\n",
    "            value = feature_map.get(row_data[i], row_data[i])\n",
    "        elif i == 9 and row_data[i]:  # Colors\n",
    "            value = \" \".join(colors_map.get(v, v) for v in row_data[i].split())\n",
    "        else:\n",
    "            value = row_data[i]\n",
    "\n",
    "        row_dict[field_name] = value\n",
    "\n",
    "    processed_rows.append(row_dict)\n",
    "\n",
    "# In convert-to-csv.py, modify the final section:\n",
    "# Write to CSV with only first 200 rows\n",
    "with open(PH2_DATASET_GENERATED_CSV_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    for row in processed_rows[:200]:  # Only write first 200 rows\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Conversion complete. CSV saved as {PH2_DATASET_GENERATED_CSV_PATH} with {len(processed_rows)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Metadata modification & merging\n",
    "This cell filters out targeted columns & rows from all datasets and merging the all metadata files after preparing image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.623Z"
    },
    "id": "lJfrLLFg3S79",
    "outputId": "b92506e3-8508-4e46-ab4c-1aee72ea3610",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load datasets\n",
    "df_ham = pd.read_csv(HAM_METADATA_PATH)\n",
    "df_pad = pd.read_csv(PAD_METADATA_PATH)\n",
    "df_derm7pt = pd.read_csv(DARM_METADATA_PATH)\n",
    "df_ph2 = pd.read_csv(PH2_DATASET_GENERATED_CSV_PATH)\n",
    "\n",
    "# Assign Dataset Identifiers explicitly\n",
    "df_ham['dataset_source'] = 'HAM10000'\n",
    "df_pad['dataset_source'] = 'PAD-UFES-20'\n",
    "df_derm7pt['dataset_source'] = 'DERM7PT'\n",
    "df_ph2['dataset_source'] = 'PH2'\n",
    "\n",
    "# Explicitly standardize columns\n",
    "df_ham.rename(columns={'image_id':'image_id', 'dx':'diagnosis'}, inplace=True)\n",
    "df_pad.rename(columns={'img_id':'image_id','diagnostic':'diagnosis'}, inplace=True)\n",
    "df_derm7pt.rename(columns={'case_id':'image_id'}, inplace=True)\n",
    "df_ph2.rename(columns={'Name':'image_id'}, inplace=True)\n",
    "\n",
    "# Unified Diagnosis Mapping\n",
    "diagnosis_map = {\n",
    "    # Benign (0)\n",
    "    \"nevus\": 0, \"solar lentigo\": 0, \"dermatofibroma\": 0, \"vascular lesion\": 0, \"nev\": 0, \"sek\": 0,\n",
    "    \"seborrheic keratosis\": 0,\n",
    "    \"blue nevus\": 0, \"congenital nevus\": 0, \"dermal nevus\": 0, \"seborrheic keratosis\": 0,\n",
    "    \"nv\": 0, \"nevus\": 0, \"bkl\": 0, \"benign keratosis\": 0, \"df\": 0, \"dermatofibroma\": 0,\n",
    "    \"vasc\": 0, \"vascular\": 0,\n",
    "    \"Intradermal Nevus\": 0, \"common nevus\":0,\n",
    "    # Intermediate Benign (1)\n",
    "    \"atypical melanocytic proliferation\": 1, \"actinic keratosis\": 1, \"lichenoid keratosis\": 1,\n",
    "    \"ack\": 1, \"akiec\": 1, \"atypical nevus\":1,\n",
    "    # Intermediate Melanoma (2)\n",
    "    \"melanoma (in situ)\": 2, \"melanoma (<0.76 mm)\": 2, \"lentigo maligna\": 2,\n",
    "    \"atypical spitz tumor\": 2,\n",
    "    # Melanoma (3)\n",
    "    \"melanoma\": 3, \"melanoma metastasis\": 3, \"melanoma (>0.76 mm)\": 3,\n",
    "    \"mel\": 3, \"Nodular Melanoma\": 3, \"melanoma (0.76 to 1.5 mm)\": 3,\n",
    "    \"melanoma (more than 1.5 mm)\": 3,\n",
    "}\n",
    "\n",
    "# Explicit mapping for each dataframe individually\n",
    "df_ham['diagnosis_numeric'] = df_ham['diagnosis'].str.lower().map(diagnosis_map)\n",
    "df_pad['diagnosis_numeric'] = df_pad['diagnosis'].str.lower().map(diagnosis_map)\n",
    "df_derm7pt['diagnosis_numeric'] = df_derm7pt['diagnosis'].str.lower().map(diagnosis_map)\n",
    "df_ph2['diagnosis_numeric'] = df_ph2['Histological Diagnosis'].str.lower().map(diagnosis_map)\n",
    "\n",
    "# Merge explicitly\n",
    "unified_df = pd.concat([df_ham, df_pad, df_derm7pt, df_ph2], ignore_index=True)\n",
    "unified_df.dropna(subset=['diagnosis_numeric'], inplace=True)\n",
    "print(unified_df[\"dataset_source\"].unique())\n",
    "\n",
    "# Explicit image path generation\n",
    "def generate_image_path(row):\n",
    "    source, image_id = row['dataset_source'], row['image_id']\n",
    "\n",
    "    if source == 'HAM10000':\n",
    "        for part in ['HAM10000_images_part_1', 'HAM10000_images_part_2']:\n",
    "            path = f\"{BASE_DIR}/skin-cancer-mnist-ham10000/{part}/{image_id}.jpg\"\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "\n",
    "    elif source == 'PAD-UFES-20':\n",
    "        for part in [1,2,3]:\n",
    "            path = f\"{BASE_DIR}/skin-cancer/imgs_part_{part}/imgs_part_{part}/{image_id}\"\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "\n",
    "    elif source == 'DERM7PT':\n",
    "        if pd.notnull(row['derm']):\n",
    "            return f\"{BASE_DIR}/derm7pt/release_v0/images/{row['derm']}\"\n",
    "        elif pd.notnull(row['clinic']):\n",
    "            return f\"{BASE_DIR}/derm7pt/release_v0/images/{row['clinic']}\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    elif source == 'PH2':\n",
    "        return f\"{BASE_DIR}/ph2dataset/PH2Dataset/PH2_Dataset_images/{image_id}/{image_id}_Dermoscopic_Image/{image_id}.bmp\"\n",
    "\n",
    "unified_df['image_path'] = unified_df.apply(generate_image_path, axis=1)\n",
    "unified_df.dropna(subset=['image_path', 'diagnosis_numeric'], inplace=True)\n",
    "print(unified_df[\"dataset_source\"].unique())\n",
    "\n",
    "unified_df.to_csv(FINAL_METADATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Count Samples\n",
    "This cell counts each class samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.624Z"
    },
    "id": "808IOipPDT5p",
    "outputId": "23a5dcc5-0ee1-405d-c82a-f6a42ffee4cf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "\n",
    "DIAGNOSIS_NUMERIC = \"diagnosis_numeric\"\n",
    "\n",
    "class_to_label = {\n",
    "    0: 'Benign',\n",
    "    1: 'Intermediate Benign',\n",
    "    2: 'Intermediate Melanoma',\n",
    "    3: 'Melanoma'\n",
    "}\n",
    "\n",
    "df = pd.read_csv(FINAL_METADATA_PATH)\n",
    "value_count = df[DIAGNOSIS_NUMERIC].value_counts()\n",
    "class_counts, class_weight_dict  = {}, {}\n",
    "\n",
    "for class_num in value_count.index:\n",
    "    print(f\"{class_to_label[class_num]}: {value_count[class_num]}\")\n",
    "    class_counts[int(class_num)] = int(value_count[class_num])\n",
    "num_classes = len(value_count)\n",
    "total_samples = sum(value_count)\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Class count: {num_classes}\")\n",
    "print(f\"Class counts: {class_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Augmentation Definition\n",
    "This cell does the augmentation & saves the augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.624Z"
    },
    "id": "J8zUq6pnEIIS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def generate_augmented_df(original_df, target_count, transform, save_dir):\n",
    "    \"\"\"\n",
    "    Generates augmented images and returns new DataFrame with paths & labels.\n",
    "    Saves images to disk in save_dir.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    augmented_records = []\n",
    "    existing_count = len(original_df)\n",
    "    needed = target_count - existing_count\n",
    "\n",
    "    print(f\"Original: {existing_count}, Target: {target_count}, Augmenting: {needed}\")\n",
    "\n",
    "    augment_idx = 0\n",
    "    while len(augmented_records) < needed:\n",
    "        for idx, row in original_df.iterrows():\n",
    "            if len(augmented_records) >= needed:\n",
    "                break\n",
    "\n",
    "            img_path = row['image_path']\n",
    "            label = row['diagnosis_numeric']\n",
    "\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            augmented = transform(image=image)['image']\n",
    "\n",
    "            new_filename = f\"aug_{label}_{augment_idx}.jpg\"\n",
    "            save_path = os.path.join(save_dir, new_filename)\n",
    "            aug_img_np = augmented.permute(1, 2, 0).cpu().numpy()\n",
    "            aug_img_np = np.clip(aug_img_np * 255.0, 0, 255).astype(np.uint8)\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(aug_img_np, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            augmented_records.append({'image_path': save_path, 'diagnosis_numeric': label})\n",
    "            augment_idx += 1\n",
    "\n",
    "    new_df = pd.concat([original_df, pd.DataFrame(augmented_records)], ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "def balance_custom_classes(df, transform, save_root):\n",
    "    \"\"\"\n",
    "    Custom-balanced class augmentation strategy:\n",
    "    Benign -> 7000 (downsample)\n",
    "    Melanoma -> 5000 (augment)\n",
    "    Intermediate Benign -> 3000 (augment)\n",
    "    Intermediate Melanoma -> 1000 (augment)\n",
    "    \"\"\"\n",
    "    class_targets = {\n",
    "        0: 7000,  # Benign\n",
    "        3: 5000,  # Melanoma\n",
    "        1: 3000,  # Intermediate Benign\n",
    "        2: 1000   # Intermediate Melanoma\n",
    "    }\n",
    "\n",
    "    final_df_list = []\n",
    "\n",
    "    for cls, target_count in class_targets.items():\n",
    "        class_df = df[df['diagnosis_numeric'] == cls]\n",
    "        existing_count = len(class_df)\n",
    "\n",
    "        print(f\"\\nClass {cls}: Existing samples = {existing_count}\")\n",
    "\n",
    "        if existing_count > target_count:\n",
    "            class_df = class_df.sample(target_count, random_state=42).reset_index(drop=True)\n",
    "            print(f\"Downsampled to {target_count}\")\n",
    "            final_df_list.append(class_df)\n",
    "\n",
    "        elif existing_count < target_count:\n",
    "            save_dir = os.path.join(save_root, f\"aug_class_{cls}\")\n",
    "            class_aug_df = generate_augmented_df(class_df, target_count, transform, save_dir)\n",
    "            final_df_list.append(class_aug_df)\n",
    "\n",
    "        else:\n",
    "            final_df_list.append(class_df)\n",
    "\n",
    "    final_balanced_df = pd.concat(final_df_list, ignore_index=True)\n",
    "    return final_balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚙️ Augmentation Code Execution\n",
    "Executes a general part of the augmentation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.624Z"
    },
    "id": "qttXiCtZE_6Z",
    "outputId": "ba24c979-d33c-49e1-b4fd-90c0edc860dd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Augmentation to apply to the intermediate classes\n",
    "augment_pipeline = A.Compose([\n",
    "    A.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=30),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ColorJitter(p=0.3),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "df = pd.read_csv(FINAL_METADATA_PATH)\n",
    "\n",
    "augmented_metadata_df = balance_custom_classes(df, transform=augment_pipeline, save_root=AUGMENTED_DATA_PATH)\n",
    "augmented_metadata_df.to_csv(AUGMENTED_METADATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Count Augmented Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.626Z"
    },
    "id": "fkDqcYS0oxia",
    "outputId": "94900472-169c-40f3-c2bc-b71c882e1d7d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(augmented_metadata_df['diagnosis_numeric'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Class Weight Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.626Z"
    },
    "id": "XduKsSAFglJn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_class_weights(df, label_col='diagnosis_numeric', device='cpu'):\n",
    "    \"\"\"\n",
    "    Automatically compute class weights based on label frequency in the DataFrame.\n",
    "    \"\"\"\n",
    "    labels = df[label_col].values\n",
    "    classes = np.unique(labels)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    return torch.tensor(weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚙️ Class Weight Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.627Z"
    },
    "id": "LIbuNT5Rmetg",
    "outputId": "bec544af-b1a1-430a-da46-f9ce998b1aee",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights = calculate_class_weights(augmented_metadata_df, device=device)\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Dataset splitting\n",
    "Train: Test: Validate = 70: 15: 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.627Z"
    },
    "id": "u8flBoyPqdUb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "df = pd.read_csv(AUGMENTED_METADATA_PATH)\n",
    "\n",
    "# Split explicitly (70% train, 15% val, 15% test)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['diagnosis_numeric'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['diagnosis_numeric'], random_state=42)\n",
    "\n",
    "# Explicitly ensure integer labels after balancing\n",
    "train_df['diagnosis_numeric'] = train_df['diagnosis_numeric'].astype(int)\n",
    "val_df['diagnosis_numeric'] = val_df['diagnosis_numeric'].astype(int)\n",
    "test_df['diagnosis_numeric'] = test_df['diagnosis_numeric'].astype(int)\n",
    "\n",
    "# Save explicitly\n",
    "train_df.to_csv(TRAIN_METADATA_PATH, index=False)\n",
    "val_df.to_csv(VAL_METADATA_PATH, index=False)\n",
    "test_df.to_csv(TEST_METADATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 DataLoader Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.628Z"
    },
    "id": "9o0ZIzRGqd1Y",
    "outputId": "8f17c6cc-8740-4643-e78f-89b288bb8a10",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class SkinDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['image_path']\n",
    "        label = self.df.iloc[idx]['diagnosis_numeric']\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        return image, label\n",
    "\n",
    "transform = augment_pipeline\n",
    "\n",
    "train_dataset = SkinDataset(train_df, transform)\n",
    "val_dataset = SkinDataset(val_df, transform)\n",
    "test_dataset = SkinDataset(test_df, transform)\n",
    "\n",
    "\n",
    "# DataLoader explicitly defined\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=NUM_WORKER, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKER, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKER, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 CNN Definition with pretrained=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.629Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import densenet169\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "class DenseNet169Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(DenseNet169Classifier, self).__init__()\n",
    "        self.base = densenet169(pretrained=True)\n",
    "        in_features = self.base.classifier.in_features\n",
    "        self.base.classifier = nn.Identity()\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.base(x)\n",
    "        return self.fc(features)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DenseNet169Classifier().to(device)\n",
    "\n",
    "class_weights = class_weights.to(device)\n",
    "learning_rate = 1e-4 * (BATCH / 32)\n",
    "# learning_rate = LR\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=WEIGHT_DECAY)\n",
    "# scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=ETA_MIN)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=T_MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.629Z"
    },
    "id": "Q65YdtfLrc8Y",
    "outputId": "3adca7b8-5e21-4d73-89f7-ff729f263af6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Initialize tracking\n",
    "train_losses, val_accuracies, f1_scores = [], [], []\n",
    "best_val_accuracy = 0\n",
    "resume_path = \"denseNet169_checkpoint.pth\"\n",
    "\n",
    "# Optional: Resume previous training\n",
    "# checkpoint = torch.load(resume_path)\n",
    "# start_epoch = checkpoint['epoch']\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# train_losses = checkpoint['train_losses']\n",
    "# val_accuracies = checkpoint['val_accuracies']\n",
    "# f1_scores = checkpoint['f1_scores']\n",
    "# num_epochs = 50\n",
    "\n",
    "start_epoch = 0  # 0 if training from scratch\n",
    "num_epochs = EPOCH\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f\"Starting Epoch {epoch+1}......\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    train_accuracy = correct / total_samples\n",
    "    train_losses.append(total_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_samples = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    pred_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(1)\n",
    "\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_samples += labels.size(0)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(preds.cpu().numpy())\n",
    "            pred_probs.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    val_accuracy = val_correct / val_samples\n",
    "    precision = precision_score(true_labels, pred_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, pred_labels, average='weighted')\n",
    "    f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
    "    auc = roc_auc_score(true_labels, pred_probs, multi_class='ovr', average='weighted')\n",
    "\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}, Recall (Sensitivity): {recall:.4f}, F1-score: {f1:.4f}, AUC: {auc:.4f}')\n",
    "\n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), \"denseNet169_160_model_best.pth\")\n",
    "        print(\"✅ Saved Best Model!\")\n",
    "\n",
    "    # Save checkpoint to resume later\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'f1_scores': f1_scores\n",
    "    }, resume_path)\n",
    "\n",
    "    scheduler.step()\n",
    "    new_lr = scheduler.get_last_lr()\n",
    "    print(f\"Epoch {epoch+1}, Updated Learning Rate: {new_lr}\")\n",
    "    # train_val_dict[\"Learning Rate\"].append(new_lr)\n",
    "\n",
    "torch.save(model.state_dict(), \"denseNet169_160_model_final.pth\")\n",
    "print(\"✅ Training Completed! Final Model Saved.\")\n",
    "\n",
    "# 📈 Plot & Save metrics\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies, label=\"Val Accuracy\")\n",
    "plt.plot(f1_scores, label=\"Val F1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Validation Accuracy & F1\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_metrics_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:31:02.630Z"
    },
    "id": "f6ACbCOprgFV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "true_labels, pred_labels, pred_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)  # Convert logits to probabilities\n",
    "\n",
    "        pred_labels.extend(outputs.argmax(dim=1).cpu().numpy())  # Predicted class\n",
    "        true_labels.extend(labels.cpu().numpy())  # True class\n",
    "        pred_probs.extend(probs.cpu().numpy())  # Store all class probabilities\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "true_labels = np.array(true_labels)\n",
    "pred_labels = np.array(pred_labels)\n",
    "pred_probs = np.array(pred_probs)  # Shape (num_samples, num_classes)\n",
    "\n",
    "# Explicitly Binarize Labels for Multi-Class ROC-AUC\n",
    "true_labels_bin = label_binarize(true_labels, classes=[0, 1, 2, 3])\n",
    "\n",
    "# Compute Multi-Class ROC-AUC\n",
    "roc_auc = roc_auc_score(true_labels_bin, pred_probs, multi_class='ovr')\n",
    "print(f\"✅ **Multiclass ROC-AUC Score:** {roc_auc:.4f}\")\n",
    "\n",
    "# Generate ROC Curve (For Class 1)\n",
    "fpr, tpr, _ = roc_curve(true_labels_bin[:, 1], pred_probs[:, 1])\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0,1], [0,1], '--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.title('ROC Curve')\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "target_names = ['Benign', 'Intermediate-Benign', 'Intermediate-Malignant', 'Malignant']\n",
    "print(classification_report(true_labels, pred_labels, target_names=target_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3904493,
     "sourceId": 6785866,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4259963,
     "sourceId": 7337662,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6892992,
     "sourceId": 11062562,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
