{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:41:31.106437Z",
     "iopub.status.busy": "2025-04-07T18:41:31.106113Z",
     "iopub.status.idle": "2025-04-07T18:41:31.409988Z",
     "shell.execute_reply": "2025-04-07T18:41:31.409368Z",
     "shell.execute_reply.started": "2025-04-07T18:41:31.106411Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Cluster similar classes into common one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:41:31.411259Z",
     "iopub.status.busy": "2025-04-07T18:41:31.410909Z",
     "iopub.status.idle": "2025-04-07T18:41:31.415932Z",
     "shell.execute_reply": "2025-04-07T18:41:31.415316Z",
     "shell.execute_reply.started": "2025-04-07T18:41:31.411236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "diagnosis_map = {\n",
    "    # Benign (0)\n",
    "    \"nevus\": 0, \"solar lentigo\": 0, \"dermatofibroma\": 0, \"vascular lesion\": 0, \"nev\": 0, \"sek\": 0,\n",
    "    \"seborrheic keratosis\": 0,\n",
    "    \"blue nevus\": 0, \"congenital nevus\": 0, \"dermal nevus\": 0, \"seborrheic keratosis\": 0,\n",
    "    \"nv\": 0, \"nevus\": 0, \"bkl\": 0, \"benign keratosis\": 0, \"df\": 0, \"dermatofibroma\": 0,\n",
    "    \"vasc\": 0, \"vascular\": 0,\n",
    "    \"Intradermal Nevus\": 0, \"common nevus\":0,\n",
    "    # Intermediate Benign (1)\n",
    "    \"atypical melanocytic proliferation\": 1, \"actinic keratosis\": 1, \"lichenoid keratosis\": 1,\n",
    "    \"ack\": 1, \"akiec\": 1, \"atypical nevus\":1,\n",
    "    # Intermediate Melanoma (2)\n",
    "    \"melanoma (in situ)\": 2, \"melanoma (<0.76 mm)\": 2, \"lentigo maligna\": 2,\n",
    "    \"atypical spitz tumor\": 2,\n",
    "    # Melanoma (3)\n",
    "    \"melanoma\": 3, \"melanoma metastasis\": 3, \"melanoma (>0.76 mm)\": 3,\n",
    "    \"mel\": 3, \"Nodular Melanoma\": 3, \"melanoma (0.76 to 1.5 mm)\": 3,\n",
    "    \"melanoma (more than 1.5 mm)\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ From .TXT to .CSV (For PH2 Dataset)\n",
    "This cell does the necessary string manupulation to adapt the .CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:41:31.417871Z",
     "iopub.status.busy": "2025-04-07T18:41:31.417551Z",
     "iopub.status.idle": "2025-04-07T18:41:31.451613Z",
     "shell.execute_reply": "2025-04-07T18:41:31.450898Z",
     "shell.execute_reply.started": "2025-04-07T18:41:31.417834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. CSV saved as PH2_dataset.csv with 220 rows\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "# Legend mappings remain the same\n",
    "clinical_diag_map = {\n",
    "    \"0\": \"Common Nevus\",\n",
    "    \"1\": \"Atypical Nevus\",\n",
    "    \"2\": \"Melanoma\"\n",
    "}\n",
    "\n",
    "asymmetry_map = {\n",
    "    \"0\": \"Fully Symmetric\",\n",
    "    \"1\": \"Symetric in 1 axe\",\n",
    "    \"2\": \"Fully Asymmetric\"\n",
    "}\n",
    "\n",
    "feature_map = {\n",
    "    \"A\": \"Absent\",\n",
    "    \"AT\": \"Atypical\",\n",
    "    \"P\": \"Present\",\n",
    "    \"T\": \"Typical\"\n",
    "}\n",
    "\n",
    "colors_map = {\n",
    "    \"1\": \"White\",\n",
    "    \"2\": \"Red\",\n",
    "    \"3\": \"Light-Brown\",\n",
    "    \"4\": \"Dark-Brown\",\n",
    "    \"5\": \"Blue-Gray\",\n",
    "    \"6\": \"Black\"\n",
    "}\n",
    "\n",
    "# Read the file\n",
    "with open(\"/kaggle/input/ph2dataset/PH2Dataset/PH2_dataset.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Get the header line\n",
    "header_line = lines[0].strip()\n",
    "header_line = re.sub(r'^\\|\\||\\|\\|$', '', header_line)  # Remove leading/trailing ||\n",
    "\n",
    "# Split by both single and double pipes\n",
    "header_parts = re.split(r'\\|\\||\\|', header_line)\n",
    "header = [part.strip() for part in header_parts if part.strip()]\n",
    "\n",
    "# Process data rows\n",
    "processed_rows = []\n",
    "for line in lines[1:]:\n",
    "    if not line.strip() or line.startswith(\"||---\"):\n",
    "        continue\n",
    "\n",
    "    # Clean the line\n",
    "    clean_line = line.strip()\n",
    "    clean_line = re.sub(r'^\\|\\||\\|\\|$', '', clean_line)  # Remove leading/trailing ||\n",
    "\n",
    "    # Split by both single and double pipes\n",
    "    parts = re.split(r'\\|\\||\\|', clean_line)\n",
    "    row_data = [part.strip() for part in parts]\n",
    "\n",
    "    # Create a dictionary for this row with all columns\n",
    "    row_dict = {}\n",
    "\n",
    "    # Add data for each column, using empty string for missing values\n",
    "    for i, field_name in enumerate(header):\n",
    "        if i >= len(row_data):\n",
    "            value = \"\"\n",
    "        elif i == 2 and row_data[i]:  # Clinical Diagnosis\n",
    "            value = clinical_diag_map.get(row_data[i], row_data[i])\n",
    "        elif i == 3 and row_data[i]:  # Asymmetry\n",
    "            value = asymmetry_map.get(row_data[i], row_data[i])\n",
    "        elif i >= 4 and i <= 8 and row_data[i]:  # Features\n",
    "            value = feature_map.get(row_data[i], row_data[i])\n",
    "        elif i == 9 and row_data[i]:  # Colors\n",
    "            value = \" \".join(colors_map.get(v, v) for v in row_data[i].split())\n",
    "        else:\n",
    "            value = row_data[i]\n",
    "\n",
    "        row_dict[field_name] = value\n",
    "\n",
    "    processed_rows.append(row_dict)\n",
    "\n",
    "# In convert-to-csv.py, modify the final section:\n",
    "# Write to CSV with only first 200 rows\n",
    "with open(\"PH2_dataset.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    for row in processed_rows[:200]:  # Only write first 200 rows\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Conversion complete. CSV saved as PH2_dataset.csv with {len(processed_rows)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Define Paths & load metadata\n",
    "Sets file paths and load metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:41:31.452979Z",
     "iopub.status.busy": "2025-04-07T18:41:31.452787Z",
     "iopub.status.idle": "2025-04-07T18:41:31.557508Z",
     "shell.execute_reply": "2025-04-07T18:41:31.556870Z",
     "shell.execute_reply.started": "2025-04-07T18:41:31.452963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_dir = '/kaggle/input'\n",
    "FINAL_METADATA_PATH = \"/kaggle/working/unified_metadata.csv\"\n",
    "AUGMENTED_DATA_PATH = \"/kaggle/working/augmented_data/\"\n",
    "AUGMENTED_METADATA_PATH = \"/kaggle/working/augmented_metadata.csv\"\n",
    "TRAIN_METADATA_PATH = \"/kaggle/working/train_df.csv\"\n",
    "VAL_METADATA_PATH = \"/kaggle/working/validation_df.csv\"\n",
    "TEST_METADATA_PATH = \"/kaggle/working/test_df.csv\"\n",
    "\n",
    "# Example usage for each metadata file:\n",
    "pad_df = pd.read_csv('/kaggle/input/skin-cancer/metadata.csv')\n",
    "darm_df = pd.read_csv('/kaggle/input/derm7pt/release_v0/meta/meta.csv')\n",
    "ham_df = pd.read_csv('/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\n",
    "ph2_df = pd.read_csv('/kaggle/working/PH2_dataset.csv')\n",
    "\n",
    "ham_df.rename(columns={'image_id':'image_id', 'dx':'diagnosis'}, inplace=True)\n",
    "pad_df.rename(columns={'img_id':'image_id','diagnostic':'diagnosis'}, inplace=True)\n",
    "darm_df.rename(columns={'case_id':'image_id'}, inplace=True)\n",
    "ph2_df.rename(columns={'Name':'image_id', 'Clinical Diagnosis': 'diagnosis'}, inplace=True)\n",
    "\n",
    "ham_df['diagnosis_numeric'] = ham_df['diagnosis'].str.lower().map(diagnosis_map)\n",
    "pad_df['diagnosis_numeric'] = pad_df['diagnosis'].str.lower().map(diagnosis_map)\n",
    "darm_df['diagnosis_numeric'] = darm_df['diagnosis'].str.lower().map(diagnosis_map)\n",
    "ph2_df['diagnosis_numeric'] = ph2_df['diagnosis'].str.lower().map(diagnosis_map)\n",
    "\n",
    "ham_df['dataset_source'] = 'HAM10000'\n",
    "pad_df['dataset_source'] = 'PAD-UFES-20'\n",
    "darm_df['dataset_source'] = 'DERM7PT'\n",
    "ph2_df['dataset_source'] = 'PH2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Metadata modification & merging\n",
    "This cell filters out targeted columns & rows from all datasets and merging the all metadata files after preparing image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:41:31.558512Z",
     "iopub.status.busy": "2025-04-07T18:41:31.558305Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HAM10000' 'PAD-UFES-20' 'DERM7PT' 'PH2']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Merge explicitly\n",
    "# unified_df = pd.concat([df_isic, df_ham, df_pad, df_derm7pt, df_ph2], ignore_index=True)\n",
    "unified_df = pd.concat([ham_df, pad_df, darm_df, ph2_df], ignore_index=True)\n",
    "unified_df.dropna(subset=['diagnosis_numeric'], inplace=True)\n",
    "print(unified_df[\"dataset_source\"].unique())\n",
    "\n",
    "# Explicit image path generation\n",
    "def generate_image_path(row):\n",
    "    source, image_id = row['dataset_source'], row['image_id']\n",
    "    # BASE_DIR = '/kaggle/input'\n",
    "\n",
    "    # if source == 'ISIC':\n",
    "    #     return f\"{BASE_DIR}/all-isic-data-20240629/images/{image_id}.jpg\"\n",
    "\n",
    "    if source == 'HAM10000':\n",
    "        for part in ['HAM10000_images_part_1', 'HAM10000_images_part_2']:\n",
    "            path = f\"{base_dir}/skin-cancer-mnist-ham10000/{part}/{image_id}.jpg\"\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "\n",
    "    elif source == 'PAD-UFES-20':\n",
    "        for part in [1,2,3]:\n",
    "            path = f\"{base_dir}/skin-cancer/imgs_part_{part}/imgs_part_{part}/{image_id}\"\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "\n",
    "    elif source == 'DERM7PT':\n",
    "        if pd.notnull(row['derm']):\n",
    "            return f\"{base_dir}/derm7pt/release_v0/images/{row['derm']}\"\n",
    "        elif pd.notnull(row['clinic']):\n",
    "            return f\"{base_dir}/derm7pt/release_v0/images/{row['clinic']}\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    elif source == 'PH2':\n",
    "        return f\"{base_dir}/ph2dataset/PH2Dataset/PH2_Dataset_images/{image_id}/{image_id}_Dermoscopic_Image/{image_id}.bmp\"\n",
    "\n",
    "unified_df['image_path'] = unified_df.apply(generate_image_path, axis=1)\n",
    "unified_df.dropna(subset=['image_path', 'diagnosis_numeric'], inplace=True)\n",
    "print(unified_df[\"dataset_source\"].unique())\n",
    "columns_to_keep = ['image_id', 'diagnosis', 'diagnosis_numeric', 'dataset_source', 'image_path']\n",
    "unified_df = unified_df[columns_to_keep]\n",
    "unified_df.to_csv(FINAL_METADATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Augmentation Definition\n",
    "This cell does the augmentation & saves the augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def generate_augmented_df(original_df, target_count, transform, save_dir):\n",
    "    \"\"\"\n",
    "    Generates augmented images and returns new DataFrame with paths & labels.\n",
    "    Saves images to disk in save_dir.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    augmented_records = []\n",
    "    existing_count = len(original_df)\n",
    "    needed = target_count - existing_count\n",
    "\n",
    "    print(f\"Original: {existing_count}, Target: {target_count}, Augmenting: {needed}\")\n",
    "\n",
    "    augment_idx = 0\n",
    "    while len(augmented_records) < needed:\n",
    "        for idx, row in original_df.iterrows():\n",
    "            if len(augmented_records) >= needed:\n",
    "                break\n",
    "\n",
    "            img_path = row['image_path']\n",
    "            label = row['diagnosis_numeric']\n",
    "\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            augmented = transform(image=image)['image']\n",
    "\n",
    "            new_filename = f\"aug_{label}_{augment_idx}.jpg\"\n",
    "            save_path = os.path.join(save_dir, new_filename)\n",
    "            aug_img_np = augmented.permute(1, 2, 0).cpu().numpy()\n",
    "            aug_img_np = np.clip(aug_img_np * 255.0, 0, 255).astype(np.uint8)\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(aug_img_np, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            augmented_records.append({'image_path': save_path, 'diagnosis_numeric': label, 'original_image_path': img_path})\n",
    "            augment_idx += 1\n",
    "\n",
    "    new_df = pd.concat([original_df, pd.DataFrame(augmented_records)], ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "def balance_custom_classes(df, transform, save_root):\n",
    "    \"\"\"\n",
    "    Custom-balanced class augmentation strategy:\n",
    "    Benign -> 7000 (downsample)\n",
    "    Melanoma -> 5000 (augment)\n",
    "    Intermediate Benign -> 3000 (augment)\n",
    "    Intermediate Melanoma -> 1000 (augment)\n",
    "    \"\"\"\n",
    "    class_targets = {\n",
    "        0: 7000,  # Benign\n",
    "        3: 5000,  # Melanoma\n",
    "        1: 3000,  # Intermediate Benign\n",
    "        2: 1000   # Intermediate Melanoma\n",
    "    }\n",
    "\n",
    "    final_df_list = []\n",
    "\n",
    "    for cls, target_count in class_targets.items():\n",
    "        class_df = df[df['diagnosis_numeric'] == cls]\n",
    "        existing_count = len(class_df)\n",
    "\n",
    "        print(f\"\\nClass {cls}: Existing samples = {existing_count}\")\n",
    "\n",
    "        if existing_count > target_count:\n",
    "            class_df = class_df.sample(target_count, random_state=42).reset_index(drop=True)\n",
    "            print(f\"Downsampled to {target_count}\")\n",
    "            final_df_list.append(class_df)\n",
    "\n",
    "        elif existing_count < target_count:\n",
    "            save_dir = os.path.join(save_root, f\"aug_class_{cls}\")\n",
    "            class_aug_df = generate_augmented_df(class_df, target_count, transform, save_dir)\n",
    "            final_df_list.append(class_aug_df)\n",
    "\n",
    "        else:\n",
    "            final_df_list.append(class_df)\n",
    "\n",
    "    final_balanced_df = pd.concat(final_df_list, ignore_index=True)\n",
    "    return final_balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Augmentation Code Execution\n",
    "Executes a general part of the augmentation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Augmentation to apply to the intermediate classes\n",
    "augment_pipeline = A.Compose([\n",
    "    A.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=30),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ColorJitter(p=0.3),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "df = pd.read_csv(FINAL_METADATA_PATH)\n",
    "\n",
    "augmented_metadata_df = balance_custom_classes(df, transform=augment_pipeline, save_root=AUGMENTED_DATA_PATH)\n",
    "augmented_metadata_df.to_csv(AUGMENTED_METADATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Prompt creation function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_descriptive_prompt(row):\n",
    "    parts = []\n",
    "\n",
    "    # Diagnosis label mapping for readability\n",
    "    diagnosis_mapping = {\n",
    "        \"bkl\": \"benign keratosis-like lesion\",\n",
    "        \"nv\": \"melanocytic nevus\",\n",
    "        \"mel\": \"melanoma\",\n",
    "        \"bcc\": \"basal cell carcinoma\",\n",
    "        \"akiec\": \"actinic keratosis\",\n",
    "        \"vasc\": \"vascular lesion\",\n",
    "        \"df\": \"dermatofibroma\"\n",
    "    }\n",
    "\n",
    "    # General patient info\n",
    "    sex = str(row.get(\"sex\") or row.get(\"gender\", \"\")).strip().lower()\n",
    "    age = row.get(\"age\") or row.get(\"Age\")\n",
    "    location = row.get(\"localization\") or row.get(\"location\")\n",
    "    raw_diagnosis = row.get(\"diagnosis\") or row.get(\"dx\") or row.get(\"Histological Diagnosis\") or row.get(\"Clinical Diagnosis\")\n",
    "    diagnosis = diagnosis_mapping.get(str(raw_diagnosis).lower(), str(raw_diagnosis).lower()) if pd.notna(raw_diagnosis) else None\n",
    "\n",
    "    intro = []\n",
    "    has_patient_info = False\n",
    "\n",
    "    if sex and sex != 'nan':\n",
    "        intro.append(f\"Patient is {sex}\")\n",
    "        has_patient_info = True\n",
    "    if pd.notna(age):\n",
    "        try:\n",
    "            age_val = int(float(age))\n",
    "            if sex and sex != 'nan':\n",
    "                intro.append(f\"aged {age_val}\")\n",
    "            else:\n",
    "                intro.append(f\"Patient age is {age_val}\")\n",
    "            has_patient_info = True\n",
    "        except:\n",
    "            pass\n",
    "    if pd.notna(location):\n",
    "        intro.append(f\"with a lesion on the {location.lower()}\")\n",
    "        has_patient_info = True\n",
    "    # if pd.notna(diagnosis):\n",
    "    #     intro.append(f\"diagnosed as {diagnosis}\")\n",
    "\n",
    "    if intro:\n",
    "        if has_patient_info:\n",
    "            parts.append(\" \".join(intro) + \".\")\n",
    "        else:\n",
    "            diagnosis_phrase = [i for i in intro if \"diagnosed as\" in i]\n",
    "            if diagnosis_phrase:\n",
    "                parts.append(f\"This image shows a lesion {diagnosis_phrase[0]}.\")\n",
    "\n",
    "    # Symptoms (PAD)\n",
    "    for symptom in [\"itch\", \"hurt\", \"grew\", \"changed\", \"bleed\"]:\n",
    "        val = str(row.get(symptom, \"\")).lower()\n",
    "        if val in [\"1\", \"true\", \"yes\", \"t\", \"y\"]:\n",
    "            parts.append(f\"Patient reported that the lesion {symptom}s.\")\n",
    "\n",
    "    # PH2-specific structured features (grouped)\n",
    "    ph2_present = []\n",
    "    ph2_absent = []\n",
    "\n",
    "    for field in [\"Asymmetry\", \"Pigment Network\", \"Dots/Globules\", \"Streaks\",\n",
    "                  \"Regression Areas\", \"Blue-Whitish Veil\"]:\n",
    "        val = str(row.get(field, \"\")).strip()\n",
    "        if val:\n",
    "            if val.lower() == \"absent\":\n",
    "                ph2_absent.append(field.lower())\n",
    "            else:\n",
    "                ph2_present.append(f\"{field.lower()} is {val.lower()}\")\n",
    "\n",
    "    if pd.notna(row.get(\"Colors\")):\n",
    "        ph2_present.append(f\"colors observed include {row['Colors'].lower()}\")\n",
    "\n",
    "    if ph2_present:\n",
    "        parts.append(\"The lesion presents the following characteristics: \" + \", \".join(ph2_present) + \".\")\n",
    "    if ph2_absent:\n",
    "        parts.append(f\"Other features such as {', '.join(ph2_absent)} are absent.\")\n",
    "\n",
    "    # DARM features\n",
    "    darm_present = []\n",
    "    darm_absent = []\n",
    "\n",
    "    for field in [\"pigment_network\", \"streaks\", \"pigmentation\", \"regression_structures\",\n",
    "                  \"dots_and_globules\", \"blue_whitish_veil\", \"vascular_structures\"]:\n",
    "        if field in row and pd.notna(row[field]):\n",
    "            val = str(row[field]).strip().lower()\n",
    "            name = field.replace('_', ' ')\n",
    "            if val == \"absent\":\n",
    "                darm_absent.append(name)\n",
    "            else:\n",
    "                darm_present.append(f\"{val} {name}\")\n",
    "\n",
    "    if darm_present:\n",
    "        parts.append(f\"Dermoscopic features include {', '.join(darm_present)}.\")\n",
    "    if darm_absent:\n",
    "        parts.append(f\"Other features such as {', '.join(darm_absent)} are absent.\")\n",
    "\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def generate_text_prompts(df):\n",
    "    df = df.copy()\n",
    "    df['text_prompt'] = df.apply(create_descriptive_prompt, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Image path creation function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_image_path(row):\n",
    "    source, image_id = row['dataset_source'], row['image_id']\n",
    "\n",
    "    if source == 'HAM10000':\n",
    "        for part in ['HAM10000_images_part_1', 'HAM10000_images_part_2']:\n",
    "            path = f\"{base_dir}/skin-cancer-mnist-ham10000/{part}/{image_id}.jpg\"\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "\n",
    "    elif source == 'PAD-UFES-20':\n",
    "        for part in [1,2,3]:\n",
    "            path = f\"{base_dir}/skin-cancer/imgs_part_{part}/imgs_part_{part}/{image_id}\"\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "\n",
    "    elif source == 'DERM7PT':\n",
    "        if pd.notnull(row['derm']):\n",
    "            return f\"{base_dir}/derm7pt/release_v0/images/{row['derm']}\"\n",
    "        elif pd.notnull(row['clinic']):\n",
    "            return f\"{base_dir}/derm7pt/release_v0/images/{row['clinic']}\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    elif source == 'PH2':\n",
    "        return f\"{base_dir}/ph2dataset/PH2Dataset/PH2_Dataset_images/{image_id}/{image_id}_Dermoscopic_Image/{image_id}.bmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Augmentation, prompt creation & metadata finalization execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pad_df = generate_text_prompts(pad_df)\n",
    "darm_df = generate_text_prompts(darm_df)\n",
    "ham_df = generate_text_prompts(ham_df)\n",
    "ph2_df = generate_text_prompts(ph2_df)\n",
    "\n",
    "# Combine all into one final DataFrame\n",
    "original_text_prompt_df = pd.concat([pad_df, darm_df, ham_df, ph2_df], ignore_index=True)\n",
    "original_text_prompt_df['image_path'] = original_text_prompt_df.apply(generate_image_path, axis=1)\n",
    "print(\"Unified dataset preview:\")\n",
    "print(original_text_prompt_df[['image_id', 'text_prompt', 'image_path']].head().to_string())\n",
    "original_text_prompt_df = original_text_prompt_df.dropna(subset=['diagnosis_numeric'])\n",
    "\n",
    "augmented_metadata_df = pd.read_csv(AUGMENTED_METADATA_PATH)\n",
    "\n",
    "augmented_metadata_df.loc[augmented_metadata_df['original_image_path'].isna() |\n",
    "                         (augmented_metadata_df['original_image_path'] == ''),\n",
    "                         'original_image_path'] = augmented_metadata_df['image_path']\n",
    "\n",
    "merge_df = augmented_metadata_df.merge(\n",
    "    original_text_prompt_df[['image_path', 'text_prompt']],\n",
    "    left_on='original_image_path',\n",
    "    right_on='image_path',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "augmented_text_prompt_df = merge_df[['diagnosis', 'diagnosis_numeric', 'dataset_source', 'original_image_path', 'text_prompt']]\n",
    "augmented_text_prompt_df = augmented_text_prompt_df.rename(columns={\n",
    "    'original_image_path': 'image_path'\n",
    "})\n",
    "\n",
    "original_text_prompt_df.to_csv(\"original_vlm_with_text_prompt_image_path.csv\", index=False)\n",
    "augmented_text_prompt_df.to_csv(\"augmented_vlm_with_text_prompt_image_path.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Define HF token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"<HF TOKEN>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Dataloader & Hyperparameters definition, dataset splitting, training & test part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import densenet169\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, classification_report, confusion_matrix)\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "# === Dataset ===\n",
    "class PromptDiagnosisDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, transform=None):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.loc[idx]\n",
    "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            row[\"text_prompt\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(int(row[\"diagnosis_numeric\"]), dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# === Vision Encoder ===\n",
    "class DenseNet169FeatureExtractor(nn.Module):\n",
    "    def __init__(self, checkpoint_path=None):\n",
    "        super().__init__()\n",
    "        self.backbone = models.densenet169(pretrained=False)\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "\n",
    "        if checkpoint_path:\n",
    "            state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "            # Remove \"base.\" prefix if present in state_dict\n",
    "            new_state_dict = {}\n",
    "            for k, v in state_dict.items():\n",
    "                new_k = k.replace(\"base.\", \"\") if k.startswith(\"base.\") else k\n",
    "                new_state_dict[new_k] = v\n",
    "\n",
    "            self.backbone.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "\n",
    "# === Q-Former ===\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(self, query_dim, vision_dim, num_queries=32):\n",
    "        super().__init__()\n",
    "        self.query_tokens = nn.Parameter(torch.randn(num_queries, query_dim))\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=query_dim, num_heads=4, batch_first=True)\n",
    "        self.linear_proj = nn.Linear(vision_dim, query_dim)\n",
    "\n",
    "    def forward(self, vision_feats):\n",
    "        B = vision_feats.size(0)\n",
    "        queries = self.query_tokens.unsqueeze(0).expand(B, -1, -1)\n",
    "        keys = self.linear_proj(vision_feats).unsqueeze(1)\n",
    "        output, _ = self.cross_attention(queries, keys, keys)\n",
    "        return output\n",
    "\n",
    "\n",
    "# === Full Model ===\n",
    "class PromptBasedVLM(nn.Module):\n",
    "    def __init__(self, vision_encoder, qformer, text_encoder, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.qformer = qformer\n",
    "        self.text_encoder = text_encoder\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        vision_feats = self.vision_encoder(image)\n",
    "        q_output = self.qformer(vision_feats)\n",
    "        q_pooled = q_output.mean(dim=1)\n",
    "\n",
    "        text_output = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_pooled = text_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "        fused = torch.cat([q_pooled, text_pooled], dim=1)\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# === Evaluation Function ===\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            image = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(image, input_ids, attention_mask)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision (macro):\", precision_score(y_true, y_pred, average='macro'))\n",
    "    print(\"Recall (macro):\", recall_score(y_true, y_pred, average='macro'))\n",
    "    print(\"F1 Score (macro):\", f1_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "\n",
    "# === Training Function ===\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, scheduler, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            image = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(image, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        print(\"Validation Metrics:\")\n",
    "        evaluate(model, val_loader, device)\n",
    "\n",
    "\n",
    "# === Setup ===\n",
    "df = pd.read_csv(\"/kaggle/working/augmented_vlm_with_text_prompt_image_path.csv\")\n",
    "labels = df['diagnosis_numeric'].values\n",
    "classes = np.unique(labels)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=labels, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['diagnosis_numeric'], random_state=42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenizer & LoRA Text Encoder\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "text_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "peft_config = LoraConfig(\n",
    "    r=8, lora_alpha=32, target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1, bias=\"none\", task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "text_encoder = get_peft_model(text_model, peft_config)\n",
    "\n",
    "# Components\n",
    "vision_checkpoint_path = \"/kaggle/input/densenet169_with_ham10k_pad_darm7pr_ph2/pytorch/default/1/denseNet169_160_model_best.pth\"\n",
    "vision_encoder = DenseNet169FeatureExtractor(checkpoint_path=vision_checkpoint_path)\n",
    "qformer = QFormer(query_dim=384, vision_dim=1664, num_queries=32)\n",
    "model = PromptBasedVLM(vision_encoder, qformer, text_encoder, hidden_dim=384, num_classes=len(classes))\n",
    "\n",
    "# DataLoader\n",
    "train_ds = PromptDiagnosisDataset(train_df, tokenizer)\n",
    "val_ds = PromptDiagnosisDataset(val_df, tokenizer)\n",
    "test_ds = PromptDiagnosisDataset(test_df, tokenizer)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)\n",
    "test_loader = DataLoader(test_ds, batch_size=16)\n",
    "\n",
    "# Loss, Optimizer, Scheduler\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Train\n",
    "train(model, train_loader, val_loader, optimizer, criterion, scheduler, DEVICE, num_epochs=10)\n",
    "\n",
    "# Test\n",
    "print(\"\\nFinal Test Metrics:\")\n",
    "evaluate(model, test_loader, DEVICE)\n",
    "\n",
    "# Save Q-Former\n",
    "torch.save(qformer.state_dict(), \"qformer_weights.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3904493,
     "sourceId": 6785866,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4259963,
     "sourceId": 7337662,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6892992,
     "sourceId": 11062562,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 294962,
     "modelInstanceId": 274060,
     "sourceId": 326151,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
